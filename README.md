# awesome-big-models [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

[![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](https://opensource.org/licenses/MIT)

A collection of AWESOME things about big AI models.

## Survey

- On the Opportunities and Risk of Foundation Models
- A Roadmap to Big Model

## Models

- Imagen
- DALL·E 2
- Gato
- PaLM
- OPT-175B
- YUAN 1.0
- ERNIE 3.0
- WuDao 2.0
- Switch Transformer
- 阿里达摩院M6
- 快手1.9万亿参数推荐精排模型
- PLUG
- 鹏程盘古-α
- Pangu
- BAGUALU
- CogView
- WuDao
- BriVL
- DALL·E
- CLIP
- Switch Transformer
- **GPT-3** [[OpenAI]](https://openai.com/api/) [28 May 2020]  
    Language Models are Few-Shot Learners [[NeurIPS'20]](https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)  

    ```yaml
    Field: NLP  
    Params: 175 B  
    Training Data: ~680 B Tokens (45 TB)  
    Training Time: 95 A100 GPU years  
    Training Cost: $5M  
    Tags: Transformer
    ```

- Turing-NLG
- T5
- Megatron-LM
- RoBERTa
- GPT-2
- BERT
- GPT